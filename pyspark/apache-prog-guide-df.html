

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>6. Spark Programming Guide DataFrames (apache-prog-guide-df)</title>
  

  
  
    <link rel="shortcut icon" href="../_static/favicon-umich.ico"/>
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../_static/css/my_theme.css" type="text/css" />
  

  

  
    <link rel="top" title="" href="../index.html"/>
        <link rel="up" title="PySpark Notes (top-pyspark.rst)" href="top-pyspark.html"/>
        <link rel="next" title="7. Databrick Doc (databricks.rst)" href="databricks.html"/>
        <link rel="prev" title="5. (todo) Spark Programming Guide (apache-prog-guide-rdd)" href="apache-prog-guide-rdd.html"/> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> Coding Notebook
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
                <p class="caption"><span class="caption-text">Table of Contents (PySpark)</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="top-pyspark.html">PySpark Notes (<code class="docutils literal"><span class="pre">top-pyspark.rst</span></code>)</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="pyspark-snippet.html">1. <code class="docutils literal"><span class="pre">pyspark-snippet.rst</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="pyspark-snippet.html#modules">1.1. Modules</a></li>
<li class="toctree-l3"><a class="reference internal" href="pyspark-snippet.html#create-toy-dataset">1.2. Create toy dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="pyspark-snippet.html#print-rdd-per-item">1.3. Print RDD per item</a></li>
<li class="toctree-l3"><a class="reference internal" href="pyspark-snippet.html#databrick-helper-function-displaying-all-dfs-in-the-notebook">1.4. Databrick helper function displaying all DFs in the notebook</a></li>
<li class="toctree-l3"><a class="reference internal" href="pyspark-snippet.html#get-shape-of-df-gotta-be-a-better-way">1.5. Get shape of DF (gotta be a better way)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="pyspark-practice.html">2. <code class="docutils literal"><span class="pre">pyspark-practice.rst</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="pyspark-practice.html#basics-dfs">2.1. Basics DFs</a></li>
<li class="toctree-l3"><a class="reference internal" href="pyspark-practice.html#udfs">2.2. UDFs</a></li>
<li class="toctree-l3"><a class="reference internal" href="pyspark-practice.html#sorting">2.3. Sorting</a></li>
<li class="toctree-l3"><a class="reference internal" href="pyspark-practice.html#create-df-from-list-of-tuples">2.4. Create DF from list of tuples</a></li>
<li class="toctree-l3"><a class="reference internal" href="pyspark-practice.html#groupby">2.5. groupby</a></li>
<li class="toctree-l3"><a class="reference internal" href="pyspark-practice.html#practice-with-faker-data">2.6. Practice with faker data</a><ul>
<li class="toctree-l4"><a class="reference internal" href="pyspark-practice.html#little-refrehser-on-generators">2.6.1. Little refrehser on generators</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="pyspark-practice.html#register-table-to-use-sql">2.7. Register table to use SQL</a></li>
<li class="toctree-l3"><a class="reference internal" href="pyspark-practice.html#more">2.8. More</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="pyspark-overflow.html">3. <code class="docutils literal"><span class="pre">pyspark-overflow.rst</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="pyspark-overflow.html#how-to-get-a-value-from-the-row-object-in-spark-dataframe">3.1. How to get a value from the Row object in Spark Dataframe?</a></li>
<li class="toctree-l3"><a class="reference internal" href="pyspark-overflow.html#how-to-add-a-constant-column-in-a-spark-dataframe">3.2. How to add a constant column in a Spark DataFrame?</a></li>
<li class="toctree-l3"><a class="reference internal" href="pyspark-overflow.html#add-an-empty-column-to-spark-dataframe">3.3. Add an empty column to Spark DataFrame</a></li>
<li class="toctree-l3"><a class="reference internal" href="pyspark-overflow.html#count-number-of-non-nan-entries-in-each-column-of-spark-dataframe">3.4. Count number of non-NaN entries in each column of Spark dataframe</a><ul>
<li class="toctree-l4"><a class="reference internal" href="pyspark-overflow.html#use-simple-aggregation">3.4.1. Use simple aggregation</a></li>
<li class="toctree-l4"><a class="reference internal" href="pyspark-overflow.html#use-sql-null-semantics">3.4.2. Use SQL <code class="docutils literal"><span class="pre">NULL</span></code> semantics</a></li>
<li class="toctree-l4"><a class="reference internal" href="pyspark-overflow.html#in-fractions">3.4.3. In fractions</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="pyspark-overflow.html#updating-a-dataframe-column-in-spark">3.5. Updating a dataframe column in spark</a></li>
<li class="toctree-l3"><a class="reference internal" href="pyspark-overflow.html#how-do-i-add-a-new-column-to-spark-data-frame-pyspark">3.6. How do I add a new column to spark data frame (Pyspark)?</a><ul>
<li class="toctree-l4"><a class="reference internal" href="pyspark-overflow.html#use-literals-lit">3.6.1. Use literals <code class="docutils literal"><span class="pre">lit</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="pyspark-overflow.html#transforming-an-existing-column">3.6.2. Transforming an existing column</a></li>
<li class="toctree-l4"><a class="reference internal" href="pyspark-overflow.html#using-join">3.6.3. Using <code class="docutils literal"><span class="pre">join</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="pyspark-overflow.html#using-function-udf">3.6.4. Using function/UDF</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="pyspark-overflow.html#how-to-change-dataframe-column-names-in-pyspark">3.7. How to change dataframe column names in pyspark?</a><ul>
<li class="toctree-l4"><a class="reference internal" href="pyspark-overflow.html#option-1-using-selectexpr">3.7.1. Option 1. Using selectExpr.</a></li>
<li class="toctree-l4"><a class="reference internal" href="pyspark-overflow.html#option-2-using-withcolumnrenamed">3.7.2. Option 2. Using <code class="docutils literal"><span class="pre">withColumnRenamed</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="pyspark-overflow.html#using-alias">3.7.3. Using alias</a></li>
<li class="toctree-l4"><a class="reference internal" href="pyspark-overflow.html#using-sqlcontext-sql">3.7.4. Using <code class="docutils literal"><span class="pre">sqlContext.sql</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="pyspark-overflow.html#pyspark-dataframes-way-to-enumerate-without-converting-to-pandas">3.8. PySpark DataFrames - way to enumerate without converting to Pandas?</a></li>
<li class="toctree-l3"><a class="reference internal" href="pyspark-overflow.html#ones-i-just-bookmarked-for-future-reference">3.9. Ones I just bookmarked for future reference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="pyspark-overflow.html#reshaping-pivoting-data-in-spark-rdd-and-or-spark-dataframes">3.9.1. Reshaping/Pivoting data in Spark RDD and/or Spark DataFrames</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="pyspark-install.html">4. Install PySpark (<code class="docutils literal"><span class="pre">pyspark-install</span></code>)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="pyspark-install.html#windows">4.1. windows?</a></li>
<li class="toctree-l3"><a class="reference internal" href="pyspark-install.html#retry-above-is-a-clusterfuck">4.2. Retry, above is a clusterfuck</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="apache-prog-guide-rdd.html">5. (todo) Spark Programming Guide (<code class="docutils literal"><span class="pre">apache-prog-guide-rdd</span></code>)</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="">6. Spark Programming Guide DataFrames (<code class="docutils literal"><span class="pre">apache-prog-guide-df</span></code>)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#overview">6.1. Overview</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#sql">6.1.1. SQL</a></li>
<li class="toctree-l4"><a class="reference internal" href="#datasets-and-dataframes">6.1.2. Datasets and DataFrames</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#getting-started">6.2. Getting Started</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#starting-point-sparksession">6.2.1. Starting Point: <code class="docutils literal"><span class="pre">SparkSession</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#creating-dataframes">6.2.2. Creating DataFrames</a></li>
<li class="toctree-l4"><a class="reference internal" href="#untyped-dataset-operations-aka-dataframe-operations">6.2.3. Untyped Dataset Operations (aka DataFrame Operations)</a></li>
<li class="toctree-l4"><a class="reference internal" href="#running-sql-queries-programmatically">6.2.4. Running SQL Queries Programmatically</a></li>
<li class="toctree-l4"><a class="reference internal" href="#creating-datasets-only-in-scala-java">6.2.5. Creating Datasets (only in Scala/Java)</a></li>
<li class="toctree-l4"><a class="reference internal" href="#interoperating-with-rdds">6.2.6. Interoperating with RDDs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#data-sources">6.3. Data Sources</a></li>
<li class="toctree-l3"><a class="reference internal" href="#generic-load-functions">6.4. Generic Load Functions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#default-parquet">6.4.1. Default (parquet)</a></li>
<li class="toctree-l4"><a class="reference internal" href="#manually-specifying-options">6.4.2. Manually Specifying Options</a></li>
<li class="toctree-l4"><a class="reference internal" href="#run-sql-on-files-directly">6.4.3. Run SQL on files directly</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#generic-save-functions">6.5. Generic Save Functions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#save-modes">6.5.1. Save Modes</a></li>
<li class="toctree-l4"><a class="reference internal" href="#saving-to-persistent-tables">6.5.2. __Saving to Persistent Tables</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#parquet-files">6.6. Parquet Files</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#loading-data-programmatically">6.6.1. Loading Data Programmatically</a></li>
<li class="toctree-l4"><a class="reference internal" href="#partition-discovery">6.6.2. __Partition Discovery</a></li>
<li class="toctree-l4"><a class="reference internal" href="#schema-merging">6.6.3. Schema Merging</a></li>
<li class="toctree-l4"><a class="reference internal" href="#hive-metastore-parquet-conversion">6.6.4. __Hive metastore Parquet Conversion</a></li>
<li class="toctree-l4"><a class="reference internal" href="#configuration">6.6.5. Configuration</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#json-datasets">6.7. JSON Datasets</a></li>
<li class="toctree-l3"><a class="reference internal" href="#hive-tables">6.8. Hive Tables</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#interacting-with-different-versions-of-hive-metastore">6.8.1. __Interacting with Different Versions of Hive Metastore</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#jdbc-to-other-databases">6.9. __JDBC To Other Databases</a></li>
<li class="toctree-l3"><a class="reference internal" href="#performance-tuning">6.10. Performance Tuning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#caching-data-in-memory">6.10.1. Caching Data in Memory</a></li>
<li class="toctree-l4"><a class="reference internal" href="#other-configuration-options">6.10.2. Other Configuration Options</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#distributed-sql-engine">6.11. Distributed SQL Engine</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#running-the-thrift-jdbc-odbc-server">6.11.1. __Running the Thrift JDBC/ODBC server</a></li>
<li class="toctree-l4"><a class="reference internal" href="#running-the-spark-sql-cli">6.11.2. Running the Spark SQL CLI</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#reference">6.12. Reference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#data-types">6.12.1. Data-Types</a></li>
<li class="toctree-l4"><a class="reference internal" href="#nan-semantics">6.12.2. NaN Semantics</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="databricks.html">7. Databrick Doc (<code class="docutils literal"><span class="pre">databricks.rst</span></code>)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="databricks.html#ml-stuffs">7.1. ML Stuffs</a></li>
<li class="toctree-l3"><a class="reference internal" href="databricks.html#bunch-of-notebooks">7.2. Bunch of notebooks</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="top-edx.html">EdX Course Notes (<code class="docutils literal"><span class="pre">top-edx.rst</span></code>)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="cs105_lab0.html">1. cs105_lab0</a><ul>
<li class="toctree-l3"><a class="reference internal" href="cs105_lab0.html#create-dataframe-and-filter-it">1.1. Create DataFrame and filter it</a></li>
<li class="toctree-l3"><a class="reference internal" href="cs105_lab0.html#load-a-text-file">1.2. Load a text file</a></li>
<li class="toctree-l3"><a class="reference internal" href="cs105_lab0.html#check-plotting">1.3. Check plotting</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="cs105_lab1a.html">2. cs105_lab1a</a><ul>
<li class="toctree-l3"><a class="reference internal" href="cs105_lab1a.html#spark-context">2.1. Spark Context</a></li>
<li class="toctree-l3"><a class="reference internal" href="cs105_lab1a.html#sparkcontext-and-the-driver-program-cluster-structure">2.2. SparkContext and the Driver Program (Cluster Structure)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cs105_lab1a.html#about-the-driver-program">2.2.1. About the Driver Program</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs105_lab1a.html#about-hivecontext-the-type-of-sqlcontext-for-db">2.2.2. About HiveContext (the type of sqlContext for DB)</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs105_lab1a.html#sparkcontext">2.2.3. SparkContext</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cs105_lab1a.html#using-dataframes-and-chaining-transformations">2.3. Using DataFrames and chaining transformations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cs105_lab1a.html#dataframe-and-schema">2.3.1. DataFrame and Schema</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs105_lab1a.html#ways-to-define-schemas">2.3.2. Ways to define schemas</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs105_lab1a.html#distributed-data-and-using-a-collection-to-create-a-dataframe">2.3.3. Distributed Data and using a collection to create a DataFrame</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs105_lab1a.html#query-plans-and-the-catalyst-optimizer">2.3.4. Query plans and the <code class="docutils literal"><span class="pre">Catalyst</span> <span class="pre">Optimizer</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="cs105_lab1a_coding.html">3. cs105_lab1a codes</a><ul>
<li class="toctree-l3"><a class="reference internal" href="cs105_lab1a_coding.html#exercise-overview">3.1. Exercise Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="cs105_lab1a_coding.html#create-list-of-data">3.2. Create list of Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="cs105_lab1a_coding.html#create-dataframe-from-a-list">3.3. Create DataFrame from a list</a></li>
<li class="toctree-l3"><a class="reference internal" href="cs105_lab1a_coding.html#subtract-one-from-each-the-age-row">3.4. Subtract one from each the <em>age</em> row</a></li>
<li class="toctree-l3"><a class="reference internal" href="cs105_lab1a_coding.html#apply-transformations-and-examine-query-plan">3.5. Apply transformations, and examine query plan</a></li>
<li class="toctree-l3"><a class="reference internal" href="cs105_lab1a_coding.html#use-collect-to-view-results">3.6. Use <code class="docutils literal"><span class="pre">collect</span></code> to view results</a></li>
<li class="toctree-l3"><a class="reference internal" href="cs105_lab1a_coding.html#display-helper-function-in-db">3.7. display helper function in DB</a></li>
<li class="toctree-l3"><a class="reference internal" href="cs105_lab1a_coding.html#more-actions-count-to-get-total">3.8. More actions: <code class="docutils literal"><span class="pre">count</span></code> to get total</a></li>
<li class="toctree-l3"><a class="reference internal" href="cs105_lab1a_coding.html#apply-transformation-filter-and-view-results-with-collect">3.9. Apply transformation <code class="docutils literal"><span class="pre">filter</span></code> and view results with <code class="docutils literal"><span class="pre">collect</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="cs105_lab1a_coding.html#lambda-functions-and-udfs">3.10. Lambda functions and UDFs</a></li>
<li class="toctree-l3"><a class="reference internal" href="cs105_lab1a_coding.html#additional-dataframe-actions">3.11. Additional DataFrame actions</a></li>
<li class="toctree-l3"><a class="reference internal" href="cs105_lab1a_coding.html#additional-dataframe-transformations">3.12. Additional DataFrame transformations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cs105_lab1a_coding.html#orderby">3.12.1. orderBy</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs105_lab1a_coding.html#a-distinct-and-dropduplicates">3.12.2. A <code class="docutils literal"><span class="pre">distinct</span></code> and <code class="docutils literal"><span class="pre">dropDuplicates</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="cs105_lab1a_coding.html#drop">3.12.3. drop</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs105_lab1a_coding.html#groupby">3.12.4. groupBy</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs105_lab1a_coding.html#sample">3.12.5. sample</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cs105_lab1a_coding.html#caching-dataframes-and-storage-options">3.13. Caching DataFrames and Storage Options</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cs105_lab1a_coding.html#unpersist-and-storage-options">3.13.1. Unpersist and storage options</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cs105_lab1a_coding.html#debugging-spark-applications-and-lazy-evaluation">3.14. Debugging Spark applications and lazy evaluation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cs105_lab1a_coding.html#jvm-and-py4j-how-python-is-executed-in-spark">3.14.1. JVM and Py4J - How Python is Executed in Spark</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs105_lab1a_coding.html#challenges-with-lazy-evaluation-using-transformations-and-actions">3.14.2. Challenges with lazy evaluation using transformations and actions</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs105_lab1a_coding.html#finding-the-bug">3.14.3. Finding the bug</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs105_lab1a_coding.html#moving-toward-expert-style">3.14.4. Moving toward expert style</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="cs105_lab1b.html">4. cs105_lab1b_wordcount</a><ul>
<li class="toctree-l3"><a class="reference internal" href="cs105_lab1b.html#create-a-base-df-and-perform-operations">4.1. Create a base DF and perform operations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cs105_lab1b.html#create-a-base-df">4.1.1. Create a base DF</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs105_lab1b.html#use-df-functions-to-add-an-s">4.1.2. Use DF functions to add an &#8216;s&#8217;</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs105_lab1b.html#length-of-each-word">4.1.3. Length of each word</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cs105_lab1b.html#counting-with-spark-sql-and-dataframes">4.2. Counting with Spark SQL and DataFrames</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cs105_lab1b.html#using-groupby-and-count">4.2.1. Using groupBy and count</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cs105_lab1b.html#finding-unique-words-and-a-mean-value">4.3. Finding unique words and a mean value</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cs105_lab1b.html#unique-words">4.3.1. Unique words</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs105_lab1b.html#means-of-groups-using-dataframes">4.3.2. Means of groups using DataFrames</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cs105_lab1b.html#apply-word-count-to-a-file">4.4. Apply word count to a file</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cs105_lab1b.html#the-wordcount-function">4.4.1. The wordCount function</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs105_lab1b.html#capitalization-and-punctuation">4.4.2. Capitalization and punctuation</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs105_lab1b.html#load-a-text-file">4.4.3. Load a text file</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs105_lab1b.html#words-from-lines">4.4.4. Words from lines</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs105_lab1b.html#count-the-words">4.4.5. Count the words</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cs105_lab1b.html#random-stuffs-i-played-around-with">4.5. Random stuffs I played around with</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="cs105_lab2.html">5. cs105_lab2_apache_log</a><ul>
<li class="toctree-l3"><a class="reference internal" href="cs105_lab2.html#introduction-and-imports">5.1. Introduction and Imports</a></li>
<li class="toctree-l3"><a class="reference internal" href="cs105_lab2.html#exploratory-data-analysis">5.2. Exploratory Data Analysis</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cs105_lab2.html#b-parsing-the-log-file">5.2.1. (2b) Parsing the log file</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs105_lab2.html#data-cleaning">5.2.2. Data Cleaning</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs105_lab2.html#fix-the-rows-with-null-content-size">5.2.3. Fix the rows with null content_size</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs105_lab2.html#parsing-the-timestamp">5.2.4. Parsing the timestamp</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cs105_lab2.html#analysis-walk-through-on-the-web-server-log-file">5.3. Analysis Walk-Through on the Web Server Log File</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cs105_lab2.html#example-content-size-statistics">5.3.1. Example: Content Size Statistics</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs105_lab2.html#example-http-status-analysis">5.3.2. Example: HTTP Status Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs105_lab2.html#example-status-graphing">5.3.3. Example: Status Graphing</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs105_lab2.html#example-frequent-hosts">5.3.4. Example: Frequent Hosts</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs105_lab2.html#example-visualizing-paths">5.3.5. Example: Visualizing Paths</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs105_lab2.html#example-top-paths">5.3.6. Example: Top Paths</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cs105_lab2.html#analyzing-web-server-log-file">5.4. Analyzing Web Server Log File</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cs105_lab2.html#exercise-top-ten-error-paths-hw-problem">5.4.1. Exercise: Top Ten Error Paths (HW Problem)</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs105_lab2.html#exercise-number-of-unique-hosts-hw">5.4.2. Exercise: Number of Unique Hosts (HW)</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs105_lab2.html#exercise-number-of-unique-daily-hosts">5.4.3. Exercise: Number of Unique Daily Hosts</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs105_lab2.html#exercise-visualizing-the-number-of-unique-daily-hosts">5.4.4. Exercise: Visualizing the Number of Unique Daily Hosts</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs105_lab2.html#exercise-average-number-of-daily-requests-per-host">5.4.5. Exercise: Average Number of Daily Requests per Host</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs105_lab2.html#exercise-visualizing-the-average-daily-requests-per-unique-host">5.4.6. Exercise: Visualizing the Average Daily Requests per Unique Host</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cs105_lab2.html#exploring-404-status-codes">5.5. Exploring 404 Status Codes</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cs105_lab2.html#exercise-counting-404-response-codes">5.5.1. Exercise: Counting 404 Response Codes</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs105_lab2.html#exercise-listing-404-status-code-records">5.5.2. Exercise: Listing 404 Status Code Records</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs105_lab2.html#exercise-listing-the-top-twenty-404-response-code-paths">5.5.3. Exercise: Listing the Top Twenty 404 Response Code paths</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs105_lab2.html#exercise-listing-the-top-twenty-five-404-response-code-hosts">5.5.4. Exercise: Listing the Top Twenty-five 404 Response Code Hosts</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs105_lab2.html#exercise-listing-404-errors-per-day">5.5.5. Exercise: Listing 404 Errors per Day</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs105_lab2.html#exercise-visualizing-the-404-errors-by-day">5.5.6. Exercise: Visualizing the 404 Errors by Day</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs105_lab2.html#exercise-top-five-days-for-404-errors">5.5.7. Exercise: Top Five Days for 404 Errors</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs105_lab2.html#exercise-hourly-404-errors">5.5.8. Exercise: Hourly 404 Errors</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs105_lab2.html#exercise-visualizing-the-404-response-codes-by-hour">5.5.9. Exercise: Visualizing the 404 Response Codes by Hour</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="cs110_lab1.html">6. cs110_lab1_power_ml_pipeline</a><ul>
<li class="toctree-l3"><a class="reference internal" href="cs110_lab1.html#background">6.1. Background</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab1.html#background-power-generation">6.1.1. Background &#8212; power generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab1.html#challenge-for-power-grid-operation">6.1.2. Challenge for power grid operation</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab1.html#the-business-problem">6.1.3. The business problem</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cs110_lab1.html#business-understanding">6.2. Business Understanding</a></li>
<li class="toctree-l3"><a class="reference internal" href="cs110_lab1.html#extract-transform-load-etl-your-data">6.3. Extract-Transform-Load (ETL) Your Data</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab1.html#exercise-2a">6.3.1. Exercise 2a</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab1.html#exercise-2b">6.3.2. Exercise 2b</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab1.html#exercise-2c">6.3.3. Exercise 2c</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab1.html#exercise-2d">6.3.4. Exercise 2d</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cs110_lab1.html#explore-your-data">6.4. Explore your data</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab1.html#a">6.4.1. 3a</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab1.html#b">6.4.2. 3b</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cs110_lab1.html#visualize-your-data">6.5. Visualize your data</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab1.html#id1">6.5.1. 4a</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab1.html#exercise-4-b">6.5.2. Exercise 4(b)</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab1.html#excercise-4-c">6.5.3. Excercise 4(c)</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab1.html#exercise-4-d">6.5.4. Exercise 4(d)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cs110_lab1.html#data-preparation-where-i-left-off">6.6. Data Preparation (where i left off)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab1.html#exercise-5-a">6.6.1. Exercise 5(a)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cs110_lab1.html#data-modeling">6.7. Data Modeling</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab1.html#exercise-6-a">6.7.1. Exercise 6(a)</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab1.html#id2">6.7.2. 6(b)</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab1.html#c">6.7.3. 6(c)</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab1.html#d">6.7.4. 6(d)</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab1.html#e">6.7.5. 6e</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab1.html#f">6.7.6. 6(f)</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab1.html#g">6.7.7. 6(g)</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab1.html#h">6.7.8. 6(h)</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab1.html#i">6.7.9. 6(i)</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab1.html#j">6.7.10. 6(j)</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab1.html#k">6.7.11. 6(k)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cs110_lab1.html#tuning-and-evaluation">6.8. Tuning and Evaluation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab1.html#id3">6.8.1. 7(a)</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab1.html#exercise-7-b">6.8.2. Exercise 7(b)</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab1.html#exercise-7-c-decisiontreeregressor">6.8.3. Exercise 7(c) DecisionTreeRegressor</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab1.html#exercise-7-d">6.8.4. Exercise 7(d)</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab1.html#exercise-7-e">6.8.5. Exercise 7(e)</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab1.html#exercise-7-f-random-forest-regressor">6.8.6. Exercise 7(f) (Random Forest Regressor)</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab1.html#exercise-7-g-cross-validation">6.8.7. Exercise 7(g) (cross validation)</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab1.html#exercise-7-h-model-evaluation">6.8.8. Exercise 7(h) (model evaluation)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="cs110_lab2.html">7. cs110_lab2_als_prediction</a><ul>
<li class="toctree-l3"><a class="reference internal" href="cs110_lab2.html#preliminaries">7.1. Preliminaries</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab2.html#the-20-million-movie-sample">7.1.1. The 20-million movie sample</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab2.html#cpu-vs-io-tradeoff">7.1.2. CPU vs IO Tradeoff</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab2.html#load-and-cache">7.1.3. Load and Cache</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cs110_lab2.html#basic-recommendations">7.2. Basic Recommendations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab2.html#exercise-1a-movies-with-highest-average-ratings">7.2.1. Exercise (1a) Movies with Highest Average Ratings</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab2.html#exercise-1b-movies-with-highest-average-ratings-and-at-least-500-reviews">7.2.2. Exercise (1b) Movies with Highest Average Ratings and at least 500 reviews</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cs110_lab2.html#collaborative-filtering">7.3. Collaborative Filtering</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab2.html#exercise-2a-creating-a-training-set">7.3.1. Exercise (2a) Creating a Training Set</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab2.html#exercise-2b-alternating-least-squares">7.3.2. Exercise (2b) Alternating Least Squares</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab2.html#why-are-we-doing-our-own-cross-validation">7.3.3. Why are we doing our own cross-validation?</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab2.html#excersie-2c-testing-your-model">7.3.4. Excersie (2c) Testing Your Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab2.html#exercise-2d-comparing-your-model">7.3.5. Exercise (2d) Comparing Your Model</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cs110_lab2.html#predictions-for-yourself">7.4. Predictions for Yourself</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab2.html#exercise-3a-your-movie-ratings">7.4.1. Exercise (3a) Your Movie Ratings</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab2.html#exercise-3b-add-your-movies-to-training-dataset">7.4.2. Exercise (3b) Add Your Movies to Training Dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab2.html#exercise-3c-train-a-model-with-your-ratings">7.4.3. Exercise (3c) Train a Model with Your Ratings</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab2.html#exercise-3d-check-rmse-for-the-new-model-with-your-ratings">7.4.4. Exercise (3d) Check RMSE for the New Model with Your Ratings</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab2.html#exercise-3e-predict-your-ratings">7.4.5. Exercise (3e) Predict Your Ratings</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab2.html#exercise-3f-predict-your-ratings">7.4.6. Exercise (3f) Predict Your Ratings</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="cs110_lab3a.html">8. cs110_lab3a_word_count_rdd</a></li>
<li class="toctree-l2"><a class="reference internal" href="cs110_lab3b.html">9. cs110_lab3b_text_analysis_ER</a><ul>
<li class="toctree-l3"><a class="reference internal" href="cs110_lab3b.html#data-files">9.1. Data files</a></li>
<li class="toctree-l3"><a class="reference internal" href="cs110_lab3b.html#preliminaries">9.2. Preliminaries</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab3b.html#load-data-files">9.2.1. Load data files</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab3b.html#examine-the-lines-loaded">9.2.2. Examine the lines loaded</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cs110_lab3b.html#er-as-text-similarity-bags-of-words">9.3. ER as Text Similarity - Bags of Words</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab3b.html#exercise-1-a-tokenize-a-string">9.3.1. Exercise 1(a) Tokenize a String</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab3b.html#exercise-1b-removing-stopwords">9.3.2. Exercise (1b) removing stopwords</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab3b.html#exercise-1c-tokenizing-the-small-datasets">9.3.3. Exercise (1c) Tokenizing the small datasets</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab3b.html#exercise-1d-amazon-record-with-the-most-tokens">9.3.4. Exercise (1d) Amazon record with the most tokens</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cs110_lab3b.html#er-as-text-similarity-weighted-bag-of-words-using-tf-idf">9.4. ER as Text Similarity - Weighted Bag-of-Words using TF-IDF</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab3b.html#exercise-2a-implement-a-tf-function">9.4.1. Exercise (2a) Implement a TF function</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab3b.html#exercise-2b-create-a-corpus">9.4.2. Exercise (2b) Create a corpus</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab3b.html#exercise-2c-implement-an-idfs-function">9.4.3. Exercise (2c) Implement an IDFs function</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab3b.html#exercise-2d-tokens-with-the-smallest-idf">9.4.4. Exercise (2d) Tokens with the smallest IDF</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab3b.html#exercise-2e-idf-histogram">9.4.5. Exercise (2e) IDF Histogram</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab3b.html#exercise-2f-implement-a-tf-idf-function">9.4.6. Exercise (2f) Implement a TF-IDF function</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cs110_lab3b.html#er-as-text-similarity-cosine-similarity">9.5. ER as Text Similarity &#8212; Cosine Similarity</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab3b.html#excercise-3a-implement-the-components-of-a-cosinesimilarity-function">9.5.1. Excercise (3a) Implement the components of a cosineSimilarity function</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab3b.html#exercise-3b-implement-a-cosinesimilarity-function">9.5.2. Exercise (3b) Implement a cosineSimilarity function</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab3b.html#exercise-3c-perform-entity-resolution">9.5.3. Exercise (3c) Perform Entity Resolution</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab3b.html#exercise-3d-perform-entity-resolution-with-broadcast-variables">9.5.4. Exercise (3d) Perform Entity Resolution with Broadcast Variables</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab3b.html#e-perform-a-gold-standard-evaluation">9.5.5. (3e) Perform a Gold Standard evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab3b.html#using-the-gold-standard-data-we-can-answer-the-following-questions">9.5.6. Using the &#8220;gold standard&#8221; data we can answer the following questions</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cs110_lab3b.html#scalable-er">9.6. Scalable ER</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab3b.html#exercise-4a-tokenize-the-full-dataset">9.6.1. Exercise (4a) Tokenize the full dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab3b.html#exercise-4b-compute-idfs-and-tf-idfs-for-the-full-datasets">9.6.2. Exercise (4b) Compute IDFs and TF-IDFs for the full datasets</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab3b.html#exercise-4c-compute-norms-for-the-weights-from-the-full-datasets">9.6.3. Exercise (4c) Compute Norms for the weights from the full datasets</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab3b.html#exercise-4d-create-inverted-indices-from-the-full-datasets">9.6.4. Exercise (4d) Create inverted indices from the full datasets</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab3b.html#exercise-4e-identify-common-tokens-from-the-full-dataset">9.6.5. Exercise (4e) Identify common tokens from the full dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab3b.html#exercise-4f-identify-common-tokens-from-the-full-dataset">9.6.6. Exercise (4f) Identify common tokens from the full dataset</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cs110_lab3b.html#analysis">9.7. Analysis</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab3b.html#exercise-5a-counting-true-positives-false-positives-and-false-negatives">9.7.1. Exercise (5a) Counting True Positives, False Positives, and False Negatives</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab3b.html#b-precision-recall-and-f-measures">9.7.2. 5b Precision, Recall, and F-measures</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs110_lab3b.html#c-line-plots">9.7.3. 5c Line Plots</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="cs120_lab1a.html">10. cs120_lab1a_math_review</a><ul>
<li class="toctree-l3"><a class="reference internal" href="cs120_lab1a.html#math-review">10.1. Math review</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cs120_lab1a.html#exercise-1a-scalar-multiplication-vectors">10.1.1. Exercise (1a) Scalar multiplication: vectors</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs120_lab1a.html#exercise-1b-element-wise-multiplication-vectors">10.1.2. Exercise (1b) Element-wise multiplication: vectors</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs120_lab1a.html#exercise-1c-dot-product">10.1.3. Exercise (1c) Dot product</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs120_lab1a.html#exercise-1d-matrix-multiplication">10.1.4. Exercise (1d) Matrix multiplication</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cs120_lab1a.html#numpy">10.2. NumPy</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cs120_lab1a.html#exercise-2a-scalar-multiplication">10.2.1. Exercise (2a) Scalar multiplication</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs120_lab1a.html#exercise-2b-element-wise-multiplication-and-dot-product">10.2.2. Exercise (2b) Element-wise multiplication and dot product</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs120_lab1a.html#exercise-2c-matrix-math">10.2.3. Exercise (2c) Matrix math</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cs120_lab1a.html#additional-numpy-and-spark-linear-algebra">10.3. Additional NumPy and Spark linear algebra</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cs120_lab1a.html#exercise-3a-slices">10.3.1. Exercise (3a) Slices</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs120_lab1a.html#exercise-3b-combining-ndarray-objects">10.3.2. Exercise (3b) Combining ndarray objects</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs120_lab1a.html#exercise-3c-pyspark-s-densevector">10.3.3. Exercise (3c) PySpark&#8217;s DenseVector</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cs120_lab1a.html#python-lambda-expressions">10.4. Python lambda expressions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cs120_lab1a.html#exercise-4a-lambda-is-an-anonymous-function">10.4.1. Exercise (4a) Lambda is an anonymous function</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs120_lab1a.html#exercise-4b-lambda-fewer-steps-than-def">10.4.2. Exercise (4b) lambda fewer steps than def</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs120_lab1a.html#exercise-4c-lambda-expression-arguments">10.4.3. Exercise (4c) Lambda expression arguments</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs120_lab1a.html#exercise-4d-restrictions-on-lambda-expressions">10.4.4. Exercise (4d) Restrictions on lambda expressions</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs120_lab1a.html#exercise-4e-functional-programming">10.4.5. Exercise (4e) Functional programming</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs120_lab1a.html#exercise-4f-composability">10.4.6. Exercise (4f) Composability</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="cs120_lab1b.html">11. cs120_lab1b_word_count_rdd</a></li>
<li class="toctree-l2"><a class="reference internal" href="cs120_lab2.html">12. cs120_lab2_linear_regression_df</a></li>
<li class="toctree-l2"><a class="reference internal" href="cs120_lab3.html">13. cs120_lab3_ctr_df</a></li>
<li class="toctree-l2"><a class="reference internal" href="cs120_lab4.html">14. cs120_lab4_pca</a><ul>
<li class="toctree-l3"><a class="reference internal" href="cs120_lab4.html#work-through-the-steps-of-pca-on-a-sample-dataset">14.1. Work through the steps of PCA on a sample dataset</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cs120_lab4.html#visualization-1-two-dimensional-gaussians">14.1.1. Visualization 1: Two-dimensional Gaussians</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs120_lab4.html#a-interpreting-pca">14.1.2. 1a) Interpreting PCA</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs120_lab4.html#b-sample-covariance-matrix">14.1.3. 1b) Sample covariance matrix</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs120_lab4.html#c-covariance-function">14.1.4. 1c) Covariance Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs120_lab4.html#d-eigendecomposition">14.1.5. 1d) Eigendecomposition</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs120_lab4.html#e-pca-scores">14.1.6. 1e) PCA scores</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cs120_lab4.html#write-a-pca-function-and-evaluate-pca-on-sample-datasets">14.2. Write a PCA function and evaluate PCA on sample datasets</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cs120_lab4.html#a-pca-function">14.2.1. 2a) PCA function</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs120_lab4.html#b-pca-on-data-random">14.2.2. 2b) PCA on data_random</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs120_lab4.html#visualization-2-pca-projection">14.2.3. Visualization 2: PCA projection</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs120_lab4.html#visualization-3-three-dimensional-data">14.2.4. Visualization 3: Three-dimensional data</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs120_lab4.html#c-3d-to-2d">14.2.5. 2c) 3D to 2D</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs120_lab4.html#visualization-4-2d-representation-of-3d-data">14.2.6. Visualization 4: 2D representation of 3D data</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs120_lab4.html#d-variance-explained">14.2.7. 2d) Variance explained</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cs120_lab4.html#parse-inspect-and-preprocess-neuroscience-data-then-perform-pca">14.3. Parse, inspect, and preprocess neuroscience data then perform PCA</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cs120_lab4.html#a-load-neuroscience-data">14.3.1. 3a) Load neuroscience data</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs120_lab4.html#b-parse-the-data">14.3.2. 3b) Parse the data</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs120_lab4.html#c-min-and-max-fluorescence">14.3.3. 3c) Min and max fluorescence</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs120_lab4.html#visualization-5-pixel-intensity">14.3.4. Visualization 5: Pixel intensity</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs120_lab4.html#d-fractional-signal-change">14.3.5. 3d) Fractional signal change</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs120_lab4.html#visualization-6-normalized-data">14.3.6. Visualization 6: Normalized data</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs120_lab4.html#e-pca-on-the-scaled-data">14.3.7. 3e) PCA on the scaled data</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs120_lab4.html#visualization-7-top-two-components-as-images">14.3.8. Visualization 7: Top two components as images</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs120_lab4.html#visualization-8-top-two-components-as-one-image">14.3.9. Visualization 8: Top two components as one image</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cs120_lab4.html#feature-based-aggregation-and-pca">14.4. Feature-based aggregation and PCA</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cs120_lab4.html#a-aggregation-using-arrays">14.4.1. (4a) Aggregation using arrays</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs120_lab4.html#b-recreate-with-np-tile-and-np-eye">14.4.2. (4b) Recreate with np.tile and np.eye</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs120_lab4.html#c-recreate-with-np-kron">14.4.3. (4c) Recreate with np.kron</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs120_lab4.html#d-aggregate-by-time">14.4.4. (4d) Aggregate by time</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs120_lab4.html#e-obtain-a-compact-representation">14.4.5. (4e) Obtain a compact representation</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs120_lab4.html#visualization-9-top-two-components-by-time">14.4.6. Visualization 9: Top two components by time</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs120_lab4.html#f-aggregate-by-direction">14.4.7. (4f) Aggregate by direction</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs120_lab4.html#g-compact-representation-of-direction-data">14.4.8. (4g) Compact representation of direction data</a></li>
<li class="toctree-l4"><a class="reference internal" href="cs120_lab4.html#visualization-10-top-two-components-by-direction">14.4.9. Visualization 10: Top two components by direction</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="apache-ml-guide.html">MLLib - Main guide (<code class="docutils literal"><span class="pre">apache-ml-guide.rst</span></code>)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="apache-ml-guide.pipeline-old.html">1. Pipeline-old (<code class="docutils literal"><span class="pre">pipeline-old</span></code>)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="apache-ml-guide.pipeline-old.html#main-concepts-in-pipelines">1.1. Main concepts in Pipelines</a></li>
<li class="toctree-l3"><a class="reference internal" href="apache-ml-guide.pipeline-old.html#dataframe">1.2. DataFrame</a></li>
<li class="toctree-l3"><a class="reference internal" href="apache-ml-guide.pipeline-old.html#pipeline-components">1.3. Pipeline components</a><ul>
<li class="toctree-l4"><a class="reference internal" href="apache-ml-guide.pipeline-old.html#transformers">1.3.1. Transformers</a></li>
<li class="toctree-l4"><a class="reference internal" href="apache-ml-guide.pipeline-old.html#estimators">1.3.2. Estimators</a></li>
<li class="toctree-l4"><a class="reference internal" href="apache-ml-guide.pipeline-old.html#properties-of-pipeline-components">1.3.3. Properties of pipeline components</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="apache-ml-guide.pipeline-old.html#pipeline-transformers-and-estimators">1.4. Pipeline (transformers and estimators)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="apache-ml-guide.pipeline-old.html#how-it-works">1.4.1. How it works</a></li>
<li class="toctree-l4"><a class="reference internal" href="apache-ml-guide.pipeline-old.html#details">1.4.2. Details</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="apache-ml-guide.pipeline-old.html#parameters">1.5. Parameters</a></li>
<li class="toctree-l3"><a class="reference internal" href="apache-ml-guide.pipeline-old.html#saving-and-loading-pipelines">1.6. Saving and Loading Pipelines</a></li>
<li class="toctree-l3"><a class="reference internal" href="apache-ml-guide.pipeline-old.html#code-examples">1.7. Code Examples</a><ul>
<li class="toctree-l4"><a class="reference internal" href="apache-ml-guide.pipeline-old.html#estimator-transformer-and-param">1.7.1. Estimator, Transformer, and Param</a></li>
<li class="toctree-l4"><a class="reference internal" href="apache-ml-guide.pipeline-old.html#example-pipeline">1.7.2. Example: Pipeline</a></li>
<li class="toctree-l4"><a class="reference internal" href="apache-ml-guide.pipeline-old.html#example-model-selection-via-cv-scala">1.7.3. Example: model selection via CV (Scala)</a></li>
<li class="toctree-l4"><a class="reference internal" href="apache-ml-guide.pipeline-old.html#example-model-selection-via-train-validation-split-scala">1.7.4. Example: model selection via train validation split (scala)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="apache-ml-guide.et-fs.html">2. Extracting, transforming and selecting features (<code class="docutils literal"><span class="pre">et-fs</span></code>)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="apache-ml-guide.et-fs.html#feature-extractors">2.1. Feature Extractors</a><ul>
<li class="toctree-l4"><a class="reference internal" href="apache-ml-guide.et-fs.html#tf-idf">2.1.1. TF-IDF</a></li>
<li class="toctree-l4"><a class="reference internal" href="apache-ml-guide.et-fs.html#word2vec">2.1.2. Word2Vec</a></li>
<li class="toctree-l4"><a class="reference internal" href="apache-ml-guide.et-fs.html#countvectorizer">2.1.3. CountVectorizer</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="apache-ml-guide.et-fs.html#feature-transformers">2.2. Feature Transformers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="apache-ml-guide.et-fs.html#tokenizer">2.2.1. Tokenizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="apache-ml-guide.et-fs.html#stopwordsremover">2.2.2. StopWordsRemover</a></li>
<li class="toctree-l4"><a class="reference internal" href="apache-ml-guide.et-fs.html#n-gram">2.2.3. n-gram</a></li>
<li class="toctree-l4"><a class="reference internal" href="apache-ml-guide.et-fs.html#binarizer">2.2.4. Binarizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="apache-ml-guide.et-fs.html#pca">2.2.5. PCA</a></li>
<li class="toctree-l4"><a class="reference internal" href="apache-ml-guide.et-fs.html#polynomialexpansion">2.2.6. PolynomialExpansion</a></li>
<li class="toctree-l4"><a class="reference internal" href="apache-ml-guide.et-fs.html#discrete-cosine-transform-dct">2.2.7. Discrete Cosine Transform (DCT)</a></li>
<li class="toctree-l4"><a class="reference internal" href="apache-ml-guide.et-fs.html#stringindexer">2.2.8. StringIndexer</a></li>
<li class="toctree-l4"><a class="reference internal" href="apache-ml-guide.et-fs.html#indextostring">2.2.9. IndexToString</a></li>
<li class="toctree-l4"><a class="reference internal" href="apache-ml-guide.et-fs.html#onehotencoder">2.2.10. OneHotEncoder</a></li>
<li class="toctree-l4"><a class="reference internal" href="apache-ml-guide.et-fs.html#vectorindexer">2.2.11. VectorIndexer</a></li>
<li class="toctree-l4"><a class="reference internal" href="apache-ml-guide.et-fs.html#normalizer">2.2.12. Normalizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="apache-ml-guide.et-fs.html#standardscaler">2.2.13. StandardScaler</a></li>
<li class="toctree-l4"><a class="reference internal" href="apache-ml-guide.et-fs.html#minmaxscaler">2.2.14. MinMaxScaler</a></li>
<li class="toctree-l4"><a class="reference internal" href="apache-ml-guide.et-fs.html#maxabsscaler">2.2.15. MaxAbsScaler</a></li>
<li class="toctree-l4"><a class="reference internal" href="apache-ml-guide.et-fs.html#bucketizer">2.2.16. Bucketizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="apache-ml-guide.et-fs.html#elementwiseproduct-bm">2.2.17. ElementwiseProduct (bm)</a></li>
<li class="toctree-l4"><a class="reference internal" href="apache-ml-guide.et-fs.html#sqltransformer-bm">2.2.18. SQLTransformer (bm)</a></li>
<li class="toctree-l4"><a class="reference internal" href="apache-ml-guide.et-fs.html#vectorassembler">2.2.19. VectorAssembler</a></li>
<li class="toctree-l4"><a class="reference internal" href="apache-ml-guide.et-fs.html#quantilediscretizer">2.2.20. QuantileDiscretizer</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="apache-ml-guide.et-fs.html#feature-selectors">2.3. Feature Selectors</a><ul>
<li class="toctree-l4"><a class="reference internal" href="apache-ml-guide.et-fs.html#vectorslicer">2.3.1. VectorSlicer</a></li>
<li class="toctree-l4"><a class="reference internal" href="apache-ml-guide.et-fs.html#rformula">2.3.2. RFormula</a></li>
<li class="toctree-l4"><a class="reference internal" href="apache-ml-guide.et-fs.html#chisqselector">2.3.3. ChiSqSelector</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="apache-ml-guide.classification.html">3. Classification (<code class="docutils literal"><span class="pre">classification</span></code>)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="apache-ml-guide.classification.html#logistic-regression">3.1. Logistic regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="apache-ml-guide.classification.html#decision-tree-classifier">3.2. Decision tree classifier</a></li>
<li class="toctree-l3"><a class="reference internal" href="apache-ml-guide.classification.html#random-forest-classifier">3.3. Random forest classifier</a></li>
<li class="toctree-l3"><a class="reference internal" href="apache-ml-guide.classification.html#gradient-boosted-tree-classifier">3.4. Gradient-boosted tree classifier</a></li>
<li class="toctree-l3"><a class="reference internal" href="apache-ml-guide.classification.html#multilayer-perceptron-classifier">3.5. Multilayer perceptron classifier</a></li>
<li class="toctree-l3"><a class="reference internal" href="apache-ml-guide.classification.html#one-vs-rest-classifier-a-k-a-one-vs-all">3.6. One-vs-Rest classifier (a.k.a. One-vs-All)</a></li>
<li class="toctree-l3"><a class="reference internal" href="apache-ml-guide.classification.html#naive-bayes">3.7. Naive Bayes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="apache-ml-guide.regression.html">4. Regression (<code class="docutils literal"><span class="pre">regression.rst</span></code>)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="apache-ml-guide.regression.html#linear-regression">4.1. Linear regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="apache-ml-guide.regression.html#generalized-linear-regression">4.2. Generalized linear regression</a><ul>
<li class="toctree-l4"><a class="reference internal" href="apache-ml-guide.regression.html#available-families">4.2.1. Available families</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="apache-ml-guide.regression.html#decision-tree-regression">4.3. Decision tree regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="apache-ml-guide.regression.html#random-forest-regression">4.4. Random forest regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="apache-ml-guide.regression.html#gradient-boosted-tree-regression">4.5. Gradient-boosted tree regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="apache-ml-guide.regression.html#survival-regression">4.6. Survival regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="apache-ml-guide.regression.html#isotonic-regression">4.7. Isotonic regression</a><ul>
<li class="toctree-l4"><a class="reference internal" href="apache-ml-guide.regression.html#examples">4.7.1. Examples</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="apache-ml-guide.tree.html">5. Tree-based-methods (<code class="docutils literal"><span class="pre">tree.rst</span></code>)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="apache-ml-guide.tree.html#decision-trees">5.1. Decision trees</a><ul>
<li class="toctree-l4"><a class="reference internal" href="apache-ml-guide.tree.html#inputs-and-outputs">5.1.1. Inputs and Outputs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="apache-ml-guide.tree.html#tree-ensembles">5.2. Tree Ensembles</a><ul>
<li class="toctree-l4"><a class="reference internal" href="apache-ml-guide.tree.html#random-forests">5.2.1. Random Forests</a></li>
<li class="toctree-l4"><a class="reference internal" href="apache-ml-guide.tree.html#gradient-boosted-trees-gbts">5.2.2. Gradient-Boosted Trees (GBTs)</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="apache-ml-rdd.html">MLLib: RDD-based API Guideline (<code class="docutils literal"><span class="pre">apache-ml-rdd</span></code>)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="apache-ml-rdd.datatype.html">1. Data Types (<code class="docutils literal"><span class="pre">datatypes</span></code>)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="apache-ml-rdd.datatype.html#local-vector">1.1. Local vector</a></li>
<li class="toctree-l3"><a class="reference internal" href="apache-ml-rdd.datatype.html#labeled-point">1.2. Labeled point</a></li>
<li class="toctree-l3"><a class="reference internal" href="apache-ml-rdd.datatype.html#local-matrix">1.3. Local matrix</a></li>
<li class="toctree-l3"><a class="reference internal" href="apache-ml-rdd.datatype.html#distributed-matrix">1.4. Distributed matrix</a><ul>
<li class="toctree-l4"><a class="reference internal" href="apache-ml-rdd.datatype.html#rowmatrix">1.4.1. RowMatrix</a></li>
<li class="toctree-l4"><a class="reference internal" href="apache-ml-rdd.datatype.html#indexedrowmatrix">1.4.2. IndexedRowMatrix</a></li>
<li class="toctree-l4"><a class="reference internal" href="apache-ml-rdd.datatype.html#coordinatematrix">1.4.3. CoordinateMatrix</a></li>
<li class="toctree-l4"><a class="reference internal" href="apache-ml-rdd.datatype.html#blockmatrix">1.4.4. BlockMatrix</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="apache-ml-rdd.stats.html">2. Basic Statistics (<code class="docutils literal"><span class="pre">stats</span></code>)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="apache-ml-rdd.stats.html#summary-statistics">2.1. Summary statistics</a></li>
<li class="toctree-l3"><a class="reference internal" href="apache-ml-rdd.stats.html#correlations">2.2. Correlations</a></li>
<li class="toctree-l3"><a class="reference internal" href="apache-ml-rdd.stats.html#stratified-sampling">2.3. Stratified sampling</a></li>
<li class="toctree-l3"><a class="reference internal" href="apache-ml-rdd.stats.html#hypothesis-testing">2.4. Hypothesis testing</a><ul>
<li class="toctree-l4"><a class="reference internal" href="apache-ml-rdd.stats.html#streaming-significance-testing">2.4.1. Streaming Significance Testing</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="apache-ml-rdd.stats.html#random-data-generation">2.5. Random data generation</a></li>
<li class="toctree-l3"><a class="reference internal" href="apache-ml-rdd.stats.html#kernel-density-estimation">2.6. Kernel density estimation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="apache-ml-rdd.eval.html">3. Evaluation metrics (<code class="docutils literal"><span class="pre">eval</span></code>)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="apache-ml-rdd.eval.html#classification-model-evaluation">3.1. Classification model evaluation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="apache-ml-rdd.eval.html#binary-classification">3.1.1. Binary classification</a></li>
<li class="toctree-l4"><a class="reference internal" href="apache-ml-rdd.eval.html#multiclass-classification">3.1.2. Multiclass classification</a></li>
<li class="toctree-l4"><a class="reference internal" href="apache-ml-rdd.eval.html#multilabel-classification">3.1.3. Multilabel classification</a></li>
<li class="toctree-l4"><a class="reference internal" href="apache-ml-rdd.eval.html#ranking-systems">3.1.4. Ranking systems</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="apache-ml-rdd.eval.html#regression-model-evaluation">3.2. Regression model evaluation</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Table of Contents (Others)</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../top-sqlite.html">SQL notes (<code class="docutils literal"><span class="pre">top-sqlite.rst</span></code>)</a><ul class="simple">
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../index.html">Coding Notebook</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          





<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../index.html">Docs</a> &raquo;</li>
      
          <li><a href="top-pyspark.html">PySpark Notes (<code class="docutils literal"><span class="pre">top-pyspark.rst</span></code>)</a> &raquo;</li>
      
    <li>6. Spark Programming Guide DataFrames (<code class="docutils literal"><span class="pre">apache-prog-guide-df</span></code>)</li>
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/pyspark/apache-prog-guide-df.txt" rel="nofollow"> View page source</a>
          
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="spark-programming-guide-dataframes-apache-prog-guide-df">
<h1>6. Spark Programming Guide DataFrames (<code class="docutils literal"><span class="pre">apache-prog-guide-df</span></code>)<a class="headerlink" href="#spark-programming-guide-dataframes-apache-prog-guide-df" title="Permalink to this headline">¶</a></h1>
<p><a class="reference external" href="http://spark.apache.org/docs/latest/sql-programming-guide.html">http://spark.apache.org/docs/latest/sql-programming-guide.html</a></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">TimeStamp:</th><td class="field-body">09-05-2016 (00:55)</td>
</tr>
</tbody>
</table>
<div class="contents local topic" id="contents">
<p class="topic-title first"><cite>Contents</cite></p>
<ul class="simple">
<li><a class="reference internal" href="#overview" id="id1">Overview</a></li>
<li><a class="reference internal" href="#getting-started" id="id2">Getting Started</a></li>
<li><a class="reference internal" href="#data-sources" id="id3">Data Sources</a></li>
<li><a class="reference internal" href="#generic-load-functions" id="id4">Generic Load Functions</a></li>
<li><a class="reference internal" href="#generic-save-functions" id="id5">Generic Save Functions</a></li>
<li><a class="reference internal" href="#parquet-files" id="id6">Parquet Files</a></li>
<li><a class="reference internal" href="#json-datasets" id="id7">JSON Datasets</a></li>
<li><a class="reference internal" href="#hive-tables" id="id8">Hive Tables</a></li>
<li><a class="reference internal" href="#jdbc-to-other-databases" id="id9">__JDBC To Other Databases</a></li>
<li><a class="reference internal" href="#performance-tuning" id="id10">Performance Tuning</a></li>
<li><a class="reference internal" href="#distributed-sql-engine" id="id11">Distributed SQL Engine</a></li>
<li><a class="reference internal" href="#reference" id="id12">Reference</a></li>
</ul>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p>Below is incomplete; I only took out codes that are informative.</p>
<p class="last">I did create a <em>skeleton</em> of TOC though.</p>
</div>
<div class="section" id="overview">
<h2><a class="toc-backref" href="#id1">6.1. Overview</a><a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li>Spark SQL is a Spark module for <strong>structured</strong> data processing.</li>
<li>Spark SQL uses the extra information from the <strong>structure</strong> to perform extra optimizations (so more optimized the basic <code class="docutils literal"><span class="pre">Spark</span> <span class="pre">RDD</span> <span class="pre">API</span></code>).</li>
<li>Spark SQL also has an API that&#8217;s portable over languages (Scala, Java, Python, R)</li>
</ul>
<p>All of the examples on this page use sample data included in the Spark distribution and can be run in the <code class="docutils literal"><span class="pre">spark-shell</span></code>, <code class="docutils literal"><span class="pre">pyspark</span> <span class="pre">shell</span></code>, or <code class="docutils literal"><span class="pre">sparkR</span> <span class="pre">shell</span></code>.</p>
<div class="section" id="sql">
<h3>6.1.1. SQL<a class="headerlink" href="#sql" title="Permalink to this headline">¶</a></h3>
<p>One use of Spark SQL is to execute <strong>SQL queries</strong>.</p>
<ul class="simple">
<li>Spark SQL can also be used to read data from an <strong>existing Hive installation</strong>.<ul>
<li>See the <a class="reference internal" href="#pyspark-proguide-hive-tables"><span>Hive Tables section</span></a> for details on how to configure this feature.</li>
</ul>
</li>
<li>When running SQL from within another programming language the results will be returned as a Dataset/DataFrame.</li>
<li>You can also interact with the SQL interface using the command-line or over JDBC/ODBC.</li>
</ul>
</div>
<div class="section" id="datasets-and-dataframes">
<h3>6.1.2. Datasets and DataFrames<a class="headerlink" href="#datasets-and-dataframes" title="Permalink to this headline">¶</a></h3>
<div class="section" id="datasets">
<h4>6.1.2.1. Datasets<a class="headerlink" href="#datasets" title="Permalink to this headline">¶</a></h4>
<p>A <strong>Dataset</strong> is a distributed collection of data.</p>
<ul class="simple">
<li>Dataset is a new interface added in Spark 1.6 that provides the benefits of RDDs with the benefits of Spark SQL&#8217;s optimized execution engine.<ul>
<li><strong>Pros of RDDs</strong>: strong typing, ability to use powerful lambda functions</li>
</ul>
</li>
<li>A Dataset can be constructed from <strong>JVM objects</strong> and then manipulated using <strong>functional transformations</strong> (map, flatMap, filter, etc.).</li>
<li>The <code class="docutils literal"><span class="pre">Dataset</span> <span class="pre">API</span></code> is available in Scala and Java.</li>
</ul>
<div class="admonition important">
<p class="first admonition-title">Important</p>
<p>Python does not have the support for the Dataset API.</p>
<p class="last">But due to Python&#8217;s dynamic nature, many of the benefits of the Dataset API are already available (i.e. you can access the field of a row by name naturally row.columnName). The case for R is similar.</p>
</div>
</div>
<div class="section" id="dataframe">
<h4>6.1.2.2. DataFrame<a class="headerlink" href="#dataframe" title="Permalink to this headline">¶</a></h4>
<p>A <strong>DataFrame</strong> is a Dataset organized into named columns.</p>
<ul class="simple">
<li>It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with <strong>richer optimizations under the hood</strong>.</li>
<li>DataFrames can be constructed from a wide array of sources such as:<ul>
<li>structured data files,</li>
<li>tables in <strong>Hive</strong>,</li>
<li>external databases, or</li>
<li>existing RDDs.</li>
</ul>
</li>
<li>The <code class="docutils literal"><span class="pre">DataFrame</span> <span class="pre">API</span></code> is available in Scala, Java, Python, and R.</li>
<li>In <strong>Scala and Java</strong>, a DataFrame is represented by a Dataset of Rows.<ul>
<li>In the Scala API, DataFrame is simply a type alias of <code class="docutils literal"><span class="pre">Dataset[Row]</span></code>.</li>
<li>In the Java API, users need to use <code class="docutils literal"><span class="pre">Dataset&lt;Row&gt;</span></code> to represent a DataFrame.</li>
<li>Throughout this document, we will often refer to Scala/Java Datasets of Rows as DataFrames.</li>
</ul>
</li>
</ul>
</div>
</div>
</div>
<div class="section" id="getting-started">
<h2><a class="toc-backref" href="#id2">6.2. Getting Started</a><a class="headerlink" href="#getting-started" title="Permalink to this headline">¶</a></h2>
<div class="admonition important">
<p class="first admonition-title">Important</p>
<p class="last">Find full example code at &#8220;<code class="docutils literal"><span class="pre">examples/src/main/python/sql.py</span></code>&#8221; in the Spark repo.</p>
</div>
<div class="section" id="starting-point-sparksession">
<h3>6.2.1. Starting Point: <code class="docutils literal"><span class="pre">SparkSession</span></code><a class="headerlink" href="#starting-point-sparksession" title="Permalink to this headline">¶</a></h3>
<p>The entry point into all functionality in Spark is the <code class="docutils literal"><span class="pre">SparkSession</span></code> class.</p>
<ul class="simple">
<li>To create a basic SparkSession, just use <code class="docutils literal"><span class="pre">SparkSession.builder()</span></code>:</li>
<li>SparkSession in Spark 2.0 provides <strong>builtin support for Hive features</strong> including:<ul>
<li>the ability to write queries using <strong>HiveQL</strong>,</li>
<li>access to <strong>Hive UDFs</strong>, and</li>
<li>the ability to <strong>read data from Hive tables</strong>.</li>
</ul>
</li>
<li>you do not need to have an existing Hive setup to use these features (nice!).</li>
</ul>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>

<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span>\
    <span class="o">.</span><span class="n">builder</span>\
    <span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&quot;PythonSQL&quot;</span><span class="p">)</span>\
    <span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="s2">&quot;spark.some.config.option&quot;</span><span class="p">,</span> <span class="s2">&quot;some-value&quot;</span><span class="p">)</span>\
    <span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="creating-dataframes">
<h3>6.2.2. Creating DataFrames<a class="headerlink" href="#creating-dataframes" title="Permalink to this headline">¶</a></h3>
<p>With a <code class="docutils literal"><span class="pre">SparkSession</span></code>, applications can create DataFrames from:</p>
<ul class="simple">
<li>an existing RDD,</li>
<li>from a Hive table, or</li>
<li>from Spark data sources (see <a class="reference internal" href="#pyspark-proguide-data-sources"><span>Data Sources</span></a>).</li>
</ul>
<p>Below is an JSON file example</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="c1"># spark is an existing SparkSession</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">json</span><span class="p">(</span><span class="s2">&quot;examples/src/main/resources/people.json&quot;</span><span class="p">)</span>

<span class="c1"># Displays the content of the DataFrame to stdout</span>
<span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="untyped-dataset-operations-aka-dataframe-operations">
<h3>6.2.3. Untyped Dataset Operations (aka DataFrame Operations)<a class="headerlink" href="#untyped-dataset-operations-aka-dataframe-operations" title="Permalink to this headline">¶</a></h3>
<div class="highlight-python"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Create the DataFrame</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">json</span><span class="p">(</span><span class="s2">&quot;examples/src/main/resources/people.json&quot;</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Show the content of the DataFrame</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">age  name</span>
<span class="go">null Michael</span>
<span class="go">30   Andy</span>
<span class="go">19   Justin</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Print the schema in a tree format</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">printSchema</span><span class="p">()</span>
<span class="go">root</span>
<span class="go">|-- age: long (nullable = true)</span>
<span class="go">|-- name: string (nullable = true)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Select only the &quot;name&quot; column</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">name</span>
<span class="go">Michael</span>
<span class="go">Andy</span>
<span class="go">Justin</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Select everybody, but increment the age by 1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;name&#39;</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;age&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">name    (age + 1)</span>
<span class="go">Michael null</span>
<span class="go">Andy    31</span>
<span class="go">Justin  20</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Select people older than 21</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;age&#39;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">21</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">age name</span>
<span class="go">30  Andy</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Count people by age</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s2">&quot;age&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">count</span><span class="p">()</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="go">age  count</span>
<span class="go">null 1</span>
<span class="go">19   1</span>
<span class="go">30   1</span>
</pre></div>
</td></tr></table></div>
</div>
<div class="section" id="running-sql-queries-programmatically">
<h3>6.2.4. Running SQL Queries Programmatically<a class="headerlink" href="#running-sql-queries-programmatically" title="Permalink to this headline">¶</a></h3>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT * FROM table&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="creating-datasets-only-in-scala-java">
<h3>6.2.5. Creating Datasets (only in Scala/Java)<a class="headerlink" href="#creating-datasets-only-in-scala-java" title="Permalink to this headline">¶</a></h3>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Skipped...no python support</p>
</div>
</div>
<div class="section" id="interoperating-with-rdds">
<h3>6.2.6. Interoperating with RDDs<a class="headerlink" href="#interoperating-with-rdds" title="Permalink to this headline">¶</a></h3>
<p>Spark SQL supports two methods for converting existing RDDs into Datasets.</p>
<p>The first method uses <strong>reflection</strong> to infer the schema of an RDD that contains specific types of objects.</p>
<ul class="simple">
<li>This reflection based approach leads to <strong>more concise code</strong> and works well when you already know the schema while writing your Spark application.</li>
</ul>
<p>The second method for creating Datasets is through a <strong>programmatic interface</strong> that allows you to construct a schema and then apply it to an existing RDD.</p>
<ul class="simple">
<li>While this method is <strong>more verbose</strong>, it allows you to construct Datasets when the columns and their <strong>types are not known until runtime</strong>.</li>
</ul>
<div class="section" id="inferring-the-schema-using-reflection">
<span id="pyspark-proguide-schema-refl"></span><h4>6.2.6.1. Inferring the Schema Using Reflection<a class="headerlink" href="#inferring-the-schema-using-reflection" title="Permalink to this headline">¶</a></h4>
<p>Spark SQL can convert an RDD of <code class="docutils literal"><span class="pre">Row</span></code> objects to a DataFrame, <strong>inferring the datatypes</strong>.</p>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">Rows</span></code> are constructed by passing a <strong>list of key/value pairs</strong> as <code class="docutils literal"><span class="pre">kwargs</span></code> to the Row class (ie, pass a <code class="docutils literal"><span class="pre">dict</span></code>).<ul>
<li><strong>keys</strong>: define the column names of the table</li>
<li><strong>types</strong>: inferred by sampling the whole datase, similar to the inference that is performed on JSON files.</li>
</ul>
</li>
</ul>
<div class="highlight-python"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="c1"># spark is an existing SparkSession.</span>
<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">Row</span>
<span class="n">sc</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sparkContext</span>

<span class="c1"># Load a text file and convert each line to a Row.</span>
<span class="n">lines</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="p">(</span><span class="s2">&quot;examples/src/main/resources/people.txt&quot;</span><span class="p">)</span>
<span class="n">parts</span> <span class="o">=</span> <span class="n">lines</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">l</span><span class="p">:</span> <span class="n">l</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;,&quot;</span><span class="p">))</span>
<span class="n">people</span> <span class="o">=</span> <span class="n">parts</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="n">Row</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">p</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">age</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="mi">1</span><span class="p">])))</span>

<span class="c1"># Infer the schema, and register the DataFrame as a table.</span>
<span class="n">schemaPeople</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">people</span><span class="p">)</span>
<span class="n">schemaPeople</span><span class="o">.</span><span class="n">createOrReplaceTempView</span><span class="p">(</span><span class="s2">&quot;people&quot;</span><span class="p">)</span>

<span class="c1"># SQL can be run over DataFrames that have been registered as a table.</span>
<span class="n">teenagers</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT name FROM people WHERE age &gt;= 13 AND age &lt;= 19&quot;</span><span class="p">)</span>

<span class="c1"># The results of SQL queries are RDDs and support all the normal RDD operations.</span>
<span class="n">teenNames</span> <span class="o">=</span> <span class="n">teenagers</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="s2">&quot;Name: &quot;</span> <span class="o">+</span> <span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
<span class="k">for</span> <span class="n">teenName</span> <span class="ow">in</span> <span class="n">teenNames</span><span class="o">.</span><span class="n">collect</span><span class="p">():</span>
  <span class="k">print</span><span class="p">(</span><span class="n">teenName</span><span class="p">)</span>
</pre></div>
</td></tr></table></div>
</div>
<div class="section" id="programmatically-specifying-the-schema">
<h4>6.2.6.2. Programmatically Specifying the Schema<a class="headerlink" href="#programmatically-specifying-the-schema" title="Permalink to this headline">¶</a></h4>
<p>When a dictionary of kwargs cannot be defined ahead of time, a DataFrame can be created programmatically with <strong>three steps</strong>.</p>
<ol class="arabic simple">
<li><strong>Create an RDD of tuples or lists</strong> from the original RDD;</li>
<li>Create the schema represented by a <code class="docutils literal"><span class="pre">StructType</span></code> matching the structure of <code class="docutils literal"><span class="pre">tuples</span></code> or <code class="docutils literal"><span class="pre">lists</span></code> in the RDD created in the step 1.</li>
<li>Apply the schema to the RDD via <code class="docutils literal"><span class="pre">createDataFrame</span></code> method provided by <code class="docutils literal"><span class="pre">SparkSession</span></code>.</li>
</ol>
<p>For Example:</p>
<div class="highlight-python"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="c1"># Import SparkSession and data types</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="o">*</span>

<span class="c1"># spark is an existing SparkSession.</span>
<span class="n">sc</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sparkContext</span>

<span class="c1"># Load a text file and convert each line to a tuple.</span>
<span class="n">lines</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="p">(</span><span class="s2">&quot;examples/src/main/resources/people.txt&quot;</span><span class="p">)</span>
<span class="n">parts</span> <span class="o">=</span> <span class="n">lines</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">l</span><span class="p">:</span> <span class="n">l</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;,&quot;</span><span class="p">))</span>
<span class="n">people</span> <span class="o">=</span> <span class="n">parts</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">p</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">strip</span><span class="p">()))</span>

<span class="c1"># The schema is encoded in a string.</span>
<span class="n">schemaString</span> <span class="o">=</span> <span class="s2">&quot;name age&quot;</span>

<span class="n">fields</span> <span class="o">=</span> <span class="p">[</span><span class="n">StructField</span><span class="p">(</span><span class="n">field_name</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="bp">True</span><span class="p">)</span> <span class="k">for</span> <span class="n">field_name</span> <span class="ow">in</span> <span class="n">schemaString</span><span class="o">.</span><span class="n">split</span><span class="p">()]</span>
<span class="n">schema</span> <span class="o">=</span> <span class="n">StructType</span><span class="p">(</span><span class="n">fields</span><span class="p">)</span>

<span class="c1"># Apply the schema to the RDD.</span>
<span class="n">schemaPeople</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">people</span><span class="p">,</span> <span class="n">schema</span><span class="p">)</span>

<span class="c1"># Creates a temporary view using the DataFrame</span>
<span class="n">schemaPeople</span><span class="o">.</span><span class="n">createOrReplaceTempView</span><span class="p">(</span><span class="s2">&quot;people&quot;</span><span class="p">)</span>

<span class="c1"># SQL can be run over DataFrames that have been registered as a table.</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT name FROM people&quot;</span><span class="p">)</span>

<span class="c1"># The results of SQL queries are RDDs and support all the normal RDD operations.</span>
<span class="n">names</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="s2">&quot;Name: &quot;</span> <span class="o">+</span> <span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
<span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">names</span><span class="o">.</span><span class="n">collect</span><span class="p">():</span>
  <span class="k">print</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
</pre></div>
</td></tr></table></div>
</div>
</div>
</div>
<div class="section" id="data-sources">
<span id="pyspark-proguide-data-sources"></span><h2><a class="toc-backref" href="#id3">6.3. Data Sources</a><a class="headerlink" href="#data-sources" title="Permalink to this headline">¶</a></h2>
<p>Spark SQL supports operating on a variety of data sources through the <strong>DataFrame interface</strong>.</p>
<p>A DataFrame can be operated on using <strong>relational transformations</strong> and can also be used to create a <strong>temporary view</strong>.</p>
<div class="admonition important">
<p class="first admonition-title">Important</p>
<p class="last">Registering a DataFrame as a temporary view allows you to run SQL queries over its data.</p>
</div>
<div class="admonition-section-overview admonition">
<p class="first admonition-title">Section overview</p>
<p class="last">This section describes the general methods for loading and saving data using the Spark Data Sources and then goes into specific options that are available for the built-in data sources.</p>
</div>
</div>
<div class="section" id="generic-load-functions">
<h2><a class="toc-backref" href="#id4">6.4. Generic Load Functions</a><a class="headerlink" href="#generic-load-functions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="default-parquet">
<h3>6.4.1. Default (parquet)<a class="headerlink" href="#default-parquet" title="Permalink to this headline">¶</a></h3>
<p>The default data source (<code class="docutils literal"><span class="pre">parquet</span></code> unless otherwise configured by <code class="docutils literal"><span class="pre">spark.sql.sources.default</span></code>) will be used for all operations.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p>From <a class="reference external" href="https://parquet.apache.org/">https://parquet.apache.org/</a></p>
<blockquote>
<div>Apache Parquet is a columnar storage format available to any project in the Hadoop ecosystem, regardless of the choice of data processing framework, data model or programming language.</div></blockquote>
<p class="last">Also see <a class="reference internal" href="#pyspark-proguide-parquet-files"><span>Parquet Files</span></a></p>
</div>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;examples/src/main/resources/users.parquet&quot;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="s2">&quot;favorite_color&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;namesAndFavColors.parquet&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="manually-specifying-options">
<h3>6.4.2. Manually Specifying Options<a class="headerlink" href="#manually-specifying-options" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li>Data sources are specified by their fully qualified name (i.e., <code class="docutils literal"><span class="pre">org.apache.spark.sql.parquet</span></code>), but for built-in sources you can also use their short names (<code class="docutils literal"><span class="pre">json,</span> <span class="pre">parquet,</span> <span class="pre">jdbc</span></code>)</li>
<li>DataFrames loaded from any data source type can be converted into other types using this syntax.</li>
</ul>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;examples/src/main/resources/people.json&quot;</span><span class="p">,</span> <span class="n">format</span><span class="o">=</span><span class="s2">&quot;json&quot;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="s2">&quot;age&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;namesAndAges.parquet&quot;</span><span class="p">,</span> <span class="n">format</span><span class="o">=</span><span class="s2">&quot;parquet&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="run-sql-on-files-directly">
<h3>6.4.3. Run SQL on files directly<a class="headerlink" href="#run-sql-on-files-directly" title="Permalink to this headline">¶</a></h3>
<p>Instead of using read API to load a file into DataFrame and query it, you can also query that file directly with SQL.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT * FROM parquet.`examples/src/main/resources/users.parquet`&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="generic-save-functions">
<h2><a class="toc-backref" href="#id5">6.5. Generic Save Functions</a><a class="headerlink" href="#generic-save-functions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="save-modes">
<h3>6.5.1. Save Modes<a class="headerlink" href="#save-modes" title="Permalink to this headline">¶</a></h3>
<p>Save operations can optionally take a <code class="docutils literal"><span class="pre">SaveMode</span></code>, that specifies how to handle existing data if present.</p>
<ul class="simple">
<li>It is important to realize that these save modes <strong>do not utilize any locking</strong> and are <strong>not atomic</strong>.</li>
<li>Additionally, when performing an Overwrite, the data will be deleted before writing out the new data.</li>
</ul>
<table border="1" class="docutils">
<colgroup>
<col width="33%" />
<col width="33%" />
<col width="33%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Scala/JAVA</th>
<th class="head">Any Language</th>
<th class="head">Meaning</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><code class="docutils literal"><span class="pre">SaveMode.ErrorIfExists</span></code> (default)</td>
<td><code class="docutils literal"><span class="pre">&quot;error&quot;</span></code> (default)</td>
<td>When saving a DataFrame to a data source, if data already exists, an exception is expected to be thrown.</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">SaveMode.Append</span></code></td>
<td><code class="docutils literal"><span class="pre">&quot;append&quot;</span></code></td>
<td>When saving a DataFrame to a data source, if data or table already exists, contents of the DataFrame are expected to be appended to existing data.</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">SaveMode.Overwrite</span></code></td>
<td><code class="docutils literal"><span class="pre">&quot;overwrite&quot;</span></code></td>
<td>Overwrite mode means that when saving a DataFrame to a data source, if data/table already exists, existing data is expected to be overwritten by the contents of the DataFrame.</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">SaveMode.Ignore</span></code></td>
<td><code class="docutils literal"><span class="pre">&quot;ignore&quot;</span></code></td>
<td>Ignore mode means that when saving a DataFrame to a data source, if data already exists, the save operation is expected to not save the contents of the DataFrame and to not change the existing data. This is similar to a CREATE TABLE IF NOT EXISTS in SQL.</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="saving-to-persistent-tables">
<h3>6.5.2. __Saving to Persistent Tables<a class="headerlink" href="#saving-to-persistent-tables" title="Permalink to this headline">¶</a></h3>
<div class="admonition-todo admonition" id="index-0">
<p class="first admonition-title">Todo</p>
<p class="last">todo</p>
</div>
</div>
</div>
<div class="section" id="parquet-files">
<span id="pyspark-proguide-parquet-files"></span><h2><a class="toc-backref" href="#id6">6.6. Parquet Files</a><a class="headerlink" href="#parquet-files" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="http://parquet.io/">Parquet</a> is a columnar format that is supported by many other data processing systems.</p>
<ul class="simple">
<li>Spark SQL provides support for both reading and writing Parquet files that automatically preserves the schema of the original data.</li>
<li>When writing Parquet files, all columns are automatically converted to be nullable for compatibility reasons.</li>
</ul>
<div class="section" id="loading-data-programmatically">
<h3>6.6.1. Loading Data Programmatically<a class="headerlink" href="#loading-data-programmatically" title="Permalink to this headline">¶</a></h3>
<p>Using the data from the above example:</p>
<div class="highlight-python"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="c1"># spark from the previous example is used in this example.</span>

<span class="n">schemaPeople</span> <span class="c1"># The DataFrame from the previous example.</span>

<span class="c1"># DataFrames can be saved as Parquet files, maintaining the schema information.</span>
<span class="n">schemaPeople</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="s2">&quot;people.parquet&quot;</span><span class="p">)</span>

<span class="c1"># Read in the Parquet file created above. Parquet files are self-describing so the schema is preserved.</span>
<span class="c1"># The result of loading a parquet file is also a DataFrame.</span>
<span class="n">parquetFile</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="s2">&quot;people.parquet&quot;</span><span class="p">)</span>

<span class="c1"># Parquet files can also be used to create a temporary view and then used in SQL statements.</span>
<span class="n">parquetFile</span><span class="o">.</span><span class="n">createOrReplaceTempView</span><span class="p">(</span><span class="s2">&quot;parquetFile&quot;</span><span class="p">);</span>
<span class="n">teenagers</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT name FROM parquetFile WHERE age &gt;= 13 AND age &lt;= 19&quot;</span><span class="p">)</span>
<span class="n">teenNames</span> <span class="o">=</span> <span class="n">teenagers</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="s2">&quot;Name: &quot;</span> <span class="o">+</span> <span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
<span class="k">for</span> <span class="n">teenName</span> <span class="ow">in</span> <span class="n">teenNames</span><span class="o">.</span><span class="n">collect</span><span class="p">():</span>
  <span class="k">print</span><span class="p">(</span><span class="n">teenName</span><span class="p">)</span>
</pre></div>
</td></tr></table></div>
</div>
<div class="section" id="partition-discovery">
<h3>6.6.2. __Partition Discovery<a class="headerlink" href="#partition-discovery" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="schema-merging">
<h3>6.6.3. Schema Merging<a class="headerlink" href="#schema-merging" title="Permalink to this headline">¶</a></h3>
<p>Like ProtocolBuffer, Avro, and Thrift, Parquet also supports schema evolution. Users can start with a simple schema, and gradually add more columns to the schema as needed. In this way, users may end up with multiple Parquet files with different but mutually compatible schemas. The Parquet data source is now able to automatically detect this case and merge schemas of all these files.</p>
<p>Since schema merging is a relatively expensive operation, and is not a necessity in most cases, we turned it off by default starting from 1.5.0. You may enable it by
setting data source option mergeSchema to true when reading Parquet files (as shown in the examples below), or
setting the global SQL option spark.sql.parquet.mergeSchema to true.</p>
<div class="highlight-python"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="c1"># Create a simple DataFrame, stored into a partition directory</span>
<span class="n">df1</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>\
                                   <span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">i</span><span class="p">:</span> <span class="n">Row</span><span class="p">(</span><span class="n">single</span><span class="o">=</span><span class="n">i</span><span class="p">,</span> <span class="n">double</span><span class="o">=</span><span class="n">i</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)))</span>
<span class="n">df1</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="s2">&quot;data/test_table/key=1&quot;</span><span class="p">)</span>

<span class="c1"># Create another DataFrame in a new partition directory,</span>
<span class="c1"># adding a new column and dropping an existing column</span>
<span class="n">df2</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">11</span><span class="p">))</span>
                                   <span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">i</span><span class="p">:</span> <span class="n">Row</span><span class="p">(</span><span class="n">single</span><span class="o">=</span><span class="n">i</span><span class="p">,</span> <span class="n">triple</span><span class="o">=</span><span class="n">i</span> <span class="o">*</span> <span class="mi">3</span><span class="p">)))</span>
<span class="n">df2</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="s2">&quot;data/test_table/key=2&quot;</span><span class="p">)</span>

<span class="c1"># Read the partitioned table</span>
<span class="n">df3</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;mergeSchema&quot;</span><span class="p">,</span> <span class="s2">&quot;true&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="s2">&quot;data/test_table&quot;</span><span class="p">)</span>
<span class="n">df3</span><span class="o">.</span><span class="n">printSchema</span><span class="p">()</span>
</pre></div>
</td></tr></table></div>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="c1"># The final schema consists of all 3 columns in the Parquet files together</span>
<span class="c1"># with the partitioning column appeared in the partition directory paths.</span>
<span class="c1"># root</span>
<span class="c1"># |-- single: int (nullable = true)</span>
<span class="c1"># |-- double: int (nullable = true)</span>
<span class="c1"># |-- triple: int (nullable = true)</span>
<span class="c1"># |-- key : int (nullable = true)</span>
</pre></div>
</div>
</div>
<div class="section" id="hive-metastore-parquet-conversion">
<h3>6.6.4. __Hive metastore Parquet Conversion<a class="headerlink" href="#hive-metastore-parquet-conversion" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="configuration">
<h3>6.6.5. Configuration<a class="headerlink" href="#configuration" title="Permalink to this headline">¶</a></h3>
<p>Configuration of Parquet can be done using the <code class="docutils literal"><span class="pre">setConf</span></code> method on <code class="docutils literal"><span class="pre">SparkSession</span></code> or by running <code class="docutils literal"><span class="pre">SET</span> <span class="pre">key=value</span></code> commands using <code class="docutils literal"><span class="pre">SQL</span></code>.</p>
<table border="1" class="docutils">
<colgroup>
<col width="33%" />
<col width="33%" />
<col width="33%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Property Name</th>
<th class="head">Default</th>
<th class="head">Meaning</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><code class="docutils literal"><span class="pre">spark.sql.parquet.binaryAsString</span></code></td>
<td>false</td>
<td>Some other Parquet-producing systems, in particular Impala, Hive, and older versions of Spark SQL, do not differentiate between binary data and strings when writing out the Parquet schema. This flag tells Spark SQL to interpret binary data as a string to provide compatibility with these systems.</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">spark.sql.parquet.int96AsTimestamp</span></code></td>
<td>true</td>
<td>Some Parquet-producing systems, in particular Impala and Hive, store Timestamp into INT96. This flag tells Spark SQL to interpret INT96 data as a timestamp to provide compatibility with these systems.</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">spark.sql.parquet.cacheMetadata</span></code></td>
<td>true</td>
<td>Turns on caching of Parquet schema metadata. Can speed up querying of static data.</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">spark.sql.parquet.compression.codec</span></code></td>
<td>gzip</td>
<td>Sets the compression codec use when writing Parquet files. Acceptable values include: uncompressed, snappy, gzip, lzo.</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">spark.sql.parquet.filterPushdown</span></code></td>
<td>true</td>
<td>Enables Parquet filter push-down optimization when set to true.</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">spark.sql.hive.convertMetastoreParquet</span></code></td>
<td>true</td>
<td>When set to false, Spark SQL will use the Hive SerDe for parquet tables instead of the built in support.</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">spark.sql.parquet.mergeSchema</span></code></td>
<td>false</td>
<td>When true, the Parquet data source merges schemas collected from all data files, otherwise the schema is picked from the summary file or a random data file if no summary file is available.</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="json-datasets">
<h2><a class="toc-backref" href="#id7">6.7. JSON Datasets</a><a class="headerlink" href="#json-datasets" title="Permalink to this headline">¶</a></h2>
<div class="highlight-python"><div class="highlight"><pre><span></span>Spark SQL can automatically infer the schema of a JSON dataset and load it as a DataFrame. This conversion can be done using SparkSession.read.json on a JSON file.

Note that the file that is offered as a json file is not a typical JSON file. Each line must contain a separate, self-contained valid JSON object. As a consequence, a regular multi-line JSON file will most often fail.
</pre></div>
</div>
<div class="highlight-python"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># A JSON dataset is pointed to by path.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># The path can be either a single text file or a directory storing text files.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">people</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">json</span><span class="p">(</span><span class="s2">&quot;examples/src/main/resources/people.json&quot;</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># The inferred schema can be visualized using the printSchema() method.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">people</span><span class="o">.</span><span class="n">printSchema</span><span class="p">()</span>
<span class="go">root</span>
<span class="go">|-- age: long (nullable = true)</span>
<span class="go">|-- name: string (nullable = true)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Creates a temporary view using the DataFrame.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">people</span><span class="o">.</span><span class="n">createOrReplaceTempView</span><span class="p">(</span><span class="s2">&quot;people&quot;</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># SQL statements can be run by using the sql methods provided by `spark`.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">teenagers</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT name FROM people WHERE age &gt;= 13 AND age &lt;= 19&quot;</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Alternatively, a DataFrame can be created for a JSON dataset represented by</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># an RDD[String] storing one JSON object per string.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">anotherPeopleRDD</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">([</span>
<span class="gp">&gt;&gt;&gt; </span>  <span class="s1">&#39;{&quot;name&quot;:&quot;Yin&quot;,&quot;address&quot;:{&quot;city&quot;:&quot;Columbus&quot;,&quot;state&quot;:&quot;Ohio&quot;}}&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">anotherPeople</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">jsonRDD</span><span class="p">(</span><span class="n">anotherPeopleRDD</span><span class="p">)</span>
</pre></div>
</td></tr></table></div>
</div>
<div class="section" id="hive-tables">
<span id="pyspark-proguide-hive-tables"></span><h2><a class="toc-backref" href="#id8">6.8. Hive Tables</a><a class="headerlink" href="#hive-tables" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="http://spark.apache.org/docs/latest/sql-programming-guide.html#hive-tables">http://spark.apache.org/docs/latest/sql-programming-guide.html#hive-tables</a></p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="c1"># spark is an existing SparkSession</span>

<span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;CREATE TABLE IF NOT EXISTS src (key INT, value STRING)&quot;</span><span class="p">)</span>
<span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;LOAD DATA LOCAL INPATH &#39;examples/src/main/resources/kv1.txt&#39; INTO TABLE src&quot;</span><span class="p">)</span>

<span class="c1"># Queries can be expressed in HiveQL.</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;FROM src SELECT key, value&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
</pre></div>
</div>
<div class="section" id="interacting-with-different-versions-of-hive-metastore">
<h3>6.8.1. __Interacting with Different Versions of Hive Metastore<a class="headerlink" href="#interacting-with-different-versions-of-hive-metastore" title="Permalink to this headline">¶</a></h3>
<p><a class="reference external" href="http://spark.apache.org/docs/latest/sql-programming-guide.html#interacting-with-different-versions-of-hive-metastore">http://spark.apache.org/docs/latest/sql-programming-guide.html#interacting-with-different-versions-of-hive-metastore</a></p>
</div>
</div>
<div class="section" id="jdbc-to-other-databases">
<h2><a class="toc-backref" href="#id9">6.9. __JDBC To Other Databases</a><a class="headerlink" href="#jdbc-to-other-databases" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="http://spark.apache.org/docs/latest/sql-programming-guide.html#jdbc-to-other-databases">http://spark.apache.org/docs/latest/sql-programming-guide.html#jdbc-to-other-databases</a></p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s1">&#39;jdbc&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">options</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="s1">&#39;jdbc:postgresql:dbserver&#39;</span><span class="p">,</span> <span class="n">dbtable</span><span class="o">=</span><span class="s1">&#39;schema.tablename&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">load</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="performance-tuning">
<h2><a class="toc-backref" href="#id10">6.10. Performance Tuning</a><a class="headerlink" href="#performance-tuning" title="Permalink to this headline">¶</a></h2>
<div class="admonition important">
<p class="first admonition-title">Important</p>
<p>For some workloads it is possible to improve performance by either:</p>
<ol class="last arabic simple">
<li>caching data in memory, or by</li>
<li>turning on some experimental options.</li>
</ol>
</div>
<div class="section" id="caching-data-in-memory">
<h3>6.10.1. Caching Data in Memory<a class="headerlink" href="#caching-data-in-memory" title="Permalink to this headline">¶</a></h3>
<p>Spark SQL can cache tables using an in-memory columnar format by calling <code class="docutils literal"><span class="pre">spark.cacheTable(&quot;tableName&quot;</span></code>) or <code class="docutils literal"><span class="pre">dataFrame.cache()</span></code>.</p>
<ul class="simple">
<li>Then Spark SQL will scan only required columns and will automatically tune compression to minimize memory usage and GC pressure.</li>
<li>You can call <code class="docutils literal"><span class="pre">spark.uncacheTable(&quot;tableName&quot;)</span></code> to remove the table from memory.</li>
</ul>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Configuration of in-memory caching can be done using the <code class="docutils literal"><span class="pre">setConf</span></code> method on <code class="docutils literal"><span class="pre">SparkSession</span></code> or by running <code class="docutils literal"><span class="pre">SET</span> <span class="pre">key=value</span></code> commands using SQL.</p>
</div>
<table border="1" class="docutils">
<colgroup>
<col width="33%" />
<col width="33%" />
<col width="33%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Property Name</th>
<th class="head">Default</th>
<th class="head">Meaning</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><code class="docutils literal"><span class="pre">spark.sql.inMemoryColumnarStorage.compressed</span></code></td>
<td>true</td>
<td>When set to true Spark SQL will automatically select a compression codec for each column based on statistics of the data.</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">spark.sql.inMemoryColumnarStorage.batchSize</span></code></td>
<td>10000</td>
<td>Controls the size of batches for columnar caching. Larger batch sizes can improve memory utilization and compression, but risk OOMs when caching data</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="other-configuration-options">
<h3>6.10.2. Other Configuration Options<a class="headerlink" href="#other-configuration-options" title="Permalink to this headline">¶</a></h3>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">It is possible that these options will be deprecated in future release as more optimizations are performed automatically.</p>
</div>
<table border="1" class="docutils">
<colgroup>
<col width="33%" />
<col width="33%" />
<col width="33%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Property Name</th>
<th class="head">Default</th>
<th class="head">Meaning</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><code class="docutils literal"><span class="pre">spark.sql.files.maxPartitionBytes</span></code></td>
<td>134217728 (128 MB)</td>
<td>The maximum number of bytes to pack into a single partition when reading files.</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">spark.sql.files.openCostInBytes</span></code></td>
<td>4194304 (4 MB)</td>
<td>The estimated cost to open a file, measured by the number of bytes could be scanned in the same time. This is used when putting multiple files into a partition. It is better to over estimated, then the partitions with small files will be faster than partitions with bigger files (which is scheduled first).</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">spark.sql.autoBroadcastJoinThreshold</span></code></td>
<td>10485760 (10 MB)</td>
<td>Configures the maximum size in bytes for a table that will be broadcast to all worker nodes when performing a join. By setting this value to -1 broadcasting can be disabled. Note that currently statistics are only supported for Hive Metastore tables where the command ANALYZE TABLE &lt;tableName&gt; COMPUTE STATISTICS noscan has been run.</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">spark.sql.shuffle.partitions</span></code></td>
<td>200</td>
<td>Configures the number of partitions to use when shuffling data for joins or aggregations.</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="distributed-sql-engine">
<h2><a class="toc-backref" href="#id11">6.11. Distributed SQL Engine</a><a class="headerlink" href="#distributed-sql-engine" title="Permalink to this headline">¶</a></h2>
<p>Spark SQL can also act as a distributed query engine using its <code class="docutils literal"><span class="pre">JDBC/ODBC</span></code> or <strong>command-line interface</strong>.</p>
<p>In this mode, end-users or applications can interact with Spark SQL directly to run SQL queries, without the need to write any code.</p>
<div class="section" id="running-the-thrift-jdbc-odbc-server">
<h3>6.11.1. __Running the Thrift JDBC/ODBC server<a class="headerlink" href="#running-the-thrift-jdbc-odbc-server" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="running-the-spark-sql-cli">
<h3>6.11.2. Running the Spark SQL CLI<a class="headerlink" href="#running-the-spark-sql-cli" title="Permalink to this headline">¶</a></h3>
<p>The Spark SQL CLI is a convenient tool to run the Hive metastore service in local mode and execute queries input from the command line. Note that the Spark SQL CLI cannot talk to the Thrift JDBC server.</p>
<p>To start the Spark SQL CLI, run the following in the Spark directory:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>./bin/spark-sql
</pre></div>
</div>
<ul class="simple">
<li>To configure Hive, place your <code class="docutils literal"><span class="pre">hive-site.xml</span></code>, <code class="docutils literal"><span class="pre">core-site.xml</span></code> and <code class="docutils literal"><span class="pre">hdfs-site.xml</span></code> files in <code class="docutils literal"><span class="pre">conf/</span></code>.</li>
<li>You may run <code class="docutils literal"><span class="pre">./bin/spark-sql</span> <span class="pre">--help</span></code> for a complete list of all available options.</li>
</ul>
</div>
</div>
<div class="section" id="reference">
<h2><a class="toc-backref" href="#id12">6.12. Reference</a><a class="headerlink" href="#reference" title="Permalink to this headline">¶</a></h2>
<div class="section" id="data-types">
<h3>6.12.1. Data-Types<a class="headerlink" href="#data-types" title="Permalink to this headline">¶</a></h3>
<p><a class="reference external" href="http://spark.apache.org/docs/latest/sql-programming-guide.html#data-types">http://spark.apache.org/docs/latest/sql-programming-guide.html#data-types</a></p>
</div>
<div class="section" id="nan-semantics">
<h3>6.12.2. NaN Semantics<a class="headerlink" href="#nan-semantics" title="Permalink to this headline">¶</a></h3>
<p>There is specially handling for not-a-number (NaN) when dealing with float or double types that does not exactly match standard floating point semantics.</p>
<div class="admonition important">
<p class="first admonition-title">Important</p>
<ul class="last simple">
<li><code class="docutils literal"><span class="pre">NaN</span> <span class="pre">=</span> <span class="pre">NaN</span></code> returns true.</li>
<li>In aggregations all NaN values are grouped together.</li>
<li>NaN is treated as a normal value in join keys.</li>
<li>NaN values go last when in ascending order, larger than any other numeric value.</li>
</ul>
</div>
</div>
</div>
</div>


           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="databricks.html" class="btn btn-neutral float-right" title="7. Databrick Doc (databricks.rst)" accesskey="n">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="apache-prog-guide-rdd.html" class="btn btn-neutral" title="5. (todo) Spark Programming Guide (apache-prog-guide-rdd)" accesskey="p"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2016, Takanori Watanabe.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>
      <script type="text/javascript" src="../_static/copybutton.js"></script>
      <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>